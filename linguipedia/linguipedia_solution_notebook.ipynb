{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom pandas_profiling import ProfileReport\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport nltk\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.porter import *\n\nfrom wordcloud import WordCloud,STOPWORDS\nimport textblob as tb\n\nimport re\nimport string\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import RidgeClassifierCV, RidgeClassifier, LogisticRegression\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.metrics import roc_auc_score, precision_recall_curve\nfrom sklearn.metrics import f1_score\n\n\nfrom tqdm import tqdm\nimport time\nimport os\nimport gc\nimport psutil\n\nfrom contextlib import contextmanager\nfrom collections import defaultdict\n\nfrom scipy.sparse import hstack\nfrom scipy.sparse import csr_matrix\n\nimport lightgbm as lgb\nimport xgboost as xgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4ec21689dcf7eb35bd79e1ee715d71bf1163a24c"
      },
      "cell_type": "code",
      "source": "print(os.listdir(\"../input\"))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09332bb6ec90d0b726f9f539b790b03be34b34d6"
      },
      "cell_type": "code",
      "source": "path = '../input/'",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4fab7015258df4f840c57b7a57c29fa2d06bbafc"
      },
      "cell_type": "markdown",
      "source": "### Data import"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load train and test data\ntrain = pd.read_csv(path+'train.csv', index_col='id')\ntest = pd.read_csv(path+'test.csv', index_col='id')\n\n# Concatenate train and test in one dataframe\ndf = pd.concat([train.drop('label', axis=1), test], axis=0)\n\n# Separate labels\nlabel = train.label\n\nsubmission = pd.read_csv(path+'sample_submission.csv')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b73f115157bd4f1de0c49a5ecea35924eaa73189"
      },
      "cell_type": "code",
      "source": "print('Train data shape:{}\\nTest data shape:{}\\nFull set shape{}'.\\\n      format(train.shape, test.shape, df.shape))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "178b3be10b9d58736d9da65d1bb2ecf9bde61ecf"
      },
      "cell_type": "markdown",
      "source": "### Exploration"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3fce564ac2d081098fcd31011700929afee27710"
      },
      "cell_type": "code",
      "source": "# Check number of 0/1 labels in the dataset\nlabel.value_counts()/label.shape[0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "90ed4d257f4b7927cd30411325d50c80f9d82110"
      },
      "cell_type": "code",
      "source": "# Take a glimpse at the data\ndf.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f7807676c46cd0563290a9ed591990b0da08ca69"
      },
      "cell_type": "code",
      "source": "train[train.label==1].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "bc10ca821003bbefed439df7df588c89896b09e7"
      },
      "cell_type": "code",
      "source": "train[train.label==0].head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "022b05fd47fff8aba713f9281346a65f8fcc492a"
      },
      "cell_type": "markdown",
      "source": "### WordCloud"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "82440fe8bcbe266077a96958596beade340f6652"
      },
      "cell_type": "code",
      "source": "neg_tweets = train[train.label == 1]\nneg_string = []\nfor t in neg_tweets.tweet:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\n\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a264d5c7e17951c20d89065eb14055cf1de97fc4"
      },
      "cell_type": "code",
      "source": "neg_tweets = train[train.label == 0]\nneg_string = []\nfor t in neg_tweets.tweet:\n    neg_string.append(t)\nneg_string = pd.Series(neg_string).str.cat(sep=' ')\n\nwordcloud = WordCloud(width=1600, height=800,max_font_size=200).generate(neg_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ab5c1a7bd39e57b86cced2833559ec89260ed032"
      },
      "cell_type": "markdown",
      "source": "## Feature Engineering"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7c0145042d4b349184c36784437fa91c0153c332"
      },
      "cell_type": "code",
      "source": "def count_regexp_occ(regexp=\"\", text=None):\n    \"\"\" Simple way to get the number of occurence of a regex\"\"\"\n    return len(re.findall(regexp, text))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0bac39696d92a260faa73a9487d98390bcf6f551"
      },
      "cell_type": "code",
      "source": "# Get length in words and characters\ndf[\"raw_word_len\"] = df[\"tweet\"].apply(lambda x: len(x.split()))\ndf[\"raw_char_len\"] = df[\"tweet\"].apply(lambda x: len(x))\n# Check number of upper case, if you're angry you may write in upper case\ndf[\"nb_upper\"] = df[\"tweet\"].apply(lambda x: count_regexp_occ(r\"[A-Z]\", x))\ndf[\"caps_vs_len\"] = (df[\"nb_upper\"]/df[\"raw_char_len\"])*100\n# Check for punctuation\ndf['num_punctuation'] = df['tweet'].apply(lambda comment: sum(comment.count(w) for w in '.,;:')) \ndf[\"punc_vs_len\"] = (df[\"num_punctuation\"]/df[\"raw_char_len\"])*100\n# Check for http links\ndf[\"has_http\"] = df[\"tweet\"].apply(lambda x: count_regexp_occ(r\"http[s]{0,1}://\\S+\", x))\ndf[\"has_http\"] = df[\"has_http\"] > 0\ndf[\"has_http\"] = df[\"has_http\"].astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9b63a1ef0a85d54a6b1a5648009f7ecad6537179"
      },
      "cell_type": "code",
      "source": "df.columns",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "89cade55db8c2656560d584c6b7c251cf52e139e"
      },
      "cell_type": "code",
      "source": "numcols = ['raw_word_len', 'raw_char_len','nb_upper', 'caps_vs_len','num_punctuation', 'punc_vs_len']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9bd828acbafff6385d65f87dd5fbbc05b3b43709"
      },
      "cell_type": "code",
      "source": "df[numcols] = np.log1p(df[numcols])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6e11cc83964071b67d8b63b1cf1fa34215ed6428"
      },
      "cell_type": "markdown",
      "source": "## Clean text"
    },
    {
      "metadata": {
        "_uuid": "71bcf654757948872b1bb167f61d0156c43bccaa"
      },
      "cell_type": "markdown",
      "source": "### Remove twitter handles (@user)"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "defbedd0979f41e4e87627ba2cec30b5060849fe"
      },
      "cell_type": "code",
      "source": "def remove_pattern(text, pattern):\n    r = re.findall(pattern, text)\n    for i in r:\n        text = re.sub(i, '', text)\n    return text",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "88c989cf2309eb27139abc42c8c58df8dab19da0"
      },
      "cell_type": "code",
      "source": "df['tidy_tweet'] = np.vectorize(remove_pattern)(df['tweet'], r'@[A-Za-z0-9_]+')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9be8bc312ea2f7d7e28e0e05fb80a091ac26c167"
      },
      "cell_type": "markdown",
      "source": "### Replace english contractions"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4145296570c1d4149edd0be346de243ca08626d"
      },
      "cell_type": "code",
      "source": "cont_patterns = [\n    ('(W|w)on\\'t', 'will not'),\n    ('(C|c)an\\'t', 'can not'),\n    ('(I|i)\\'m', 'i am'),\n    ('(A|a)in\\'t', 'is not'),\n    ('(\\w+)\\'ll', '\\g<1> will'),\n    ('(\\w+)n\\'t', '\\g<1> not'),\n    ('(\\w+)\\'ve', '\\g<1> have'),\n    ('(\\w+)\\'s', '\\g<1> is'),\n    ('(\\w+)\\'re', '\\g<1> are'),\n    ('(\\w+)\\'d', '\\g<1> would'),\n]\npatterns = [(re.compile(regex), repl) for (regex, repl) in cont_patterns]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "113f29c0af519e4e4062eeb7eb4dcd9f11a60851"
      },
      "cell_type": "code",
      "source": "def clean_text(text):\n    clean = text.lower()\n    #Drop numbers\n    clean = re.sub(\"\\d+\", \" \", clean)\n    #Remove extra spaces\n    clean = re.sub('\\s+', ' ', clean)\n    # Remove ending space if any\n    clean = re.sub('\\s+$', '', clean)\n    for (pattern, repl) in patterns:\n        clean = re.sub(pattern, repl, clean)\n    return clean",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "07673bbc8e81d5fa0e000593e7c908da8f96f398"
      },
      "cell_type": "code",
      "source": "df[\"tidy_tweet\"] = df[\"tidy_tweet\"].apply(lambda x: clean_text(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed0e2b455356fddfd5c316b9195cdfbb8547cc7f"
      },
      "cell_type": "code",
      "source": "# remove http links\ndf['tidy_tweet'] = df['tidy_tweet'].str.replace(r'https?://[^ ]+', \"\")\ndf['tidy_tweet'] = df['tidy_tweet'].str.replace(r'www.[^ ]+', \"\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32964ee034e0de6686392c4f0eb6c6c5902f8c4d"
      },
      "cell_type": "code",
      "source": "# remove hashtags\n#df['tidy_tweet'] = df['tidy_tweet'].str.replace(r\"#(\\w+)\", \" \")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6d08eb795db8b625c5ea8900acf5f2115cceaf6e"
      },
      "cell_type": "code",
      "source": "# remove special characters, numbers, punctuations\ndf['tidy_tweet'] = df['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6a90538a7bedf2441a1fed92bcf7889994d742c3"
      },
      "cell_type": "code",
      "source": "# Get the new length in words and characters\ndf[\"clean_word_len\"] = df[\"tidy_tweet\"].apply(lambda x: len(x.split()))\ndf[\"clean_char_len\"] = df[\"tidy_tweet\"].apply(lambda x: len(x))\n# Number of different characters used in a comment\n# Using the f word only will reduce the number of letters required in the comment\ndf[\"clean_chars\"] = df[\"tidy_tweet\"].apply(lambda x: len(set(x)))\ndf[\"clean_chars_ratio\"] = df[\"tidy_tweet\"].apply(lambda x: len(set(x))) / df[\"tidy_tweet\"].apply(\n    lambda x: 1 + min(99, len(x)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3d14aae23d6ced60dc9215c570eb59bad1e6b6fe"
      },
      "cell_type": "markdown",
      "source": "## TF-IDF"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f8dbbadc06e0ead3cf32695a56ac33f01418ef0e"
      },
      "cell_type": "code",
      "source": "def char_analyzer(text):\n    \"\"\"\n    This is used to split strings in small lots\n    so <talk> and <talking> would have <Tal> <alk> in common\n    \"\"\"\n    tokens = text.split()\n    return [token[i: i + 3] for token in tokens for i in range(len(token) - 2)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "09451f8d3b404073c9f7880f93e0a073bac33775"
      },
      "cell_type": "code",
      "source": "# Get TF-IDF features\ntrain_text = df.iloc[:train.shape[0],:]['tidy_tweet']\ntest_text = df.iloc[train.shape[0]:,:]['tidy_tweet']\nall_text = pd.concat([train_text, test_text])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c0a88a1d68b6a219c41fff7749b2bce1581b4f83"
      },
      "cell_type": "code",
      "source": "# Use the char_analyzer to get another TFIDF\n# Char level TFIDF would go through words when char analyzer only considers\n# characters inside a word\nchar_vectorizer = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            tokenizer=char_analyzer,\n            analyzer='word',\n            ngram_range=(1, 3),\n            max_features=50000)\nchar_vectorizer.fit(all_text)\ntrain_char_features = char_vectorizer.transform(train_text)\ntest_char_features = char_vectorizer.transform(test_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11cd2608b77ab55b38a5b5a62e7759c21316d6ab"
      },
      "cell_type": "code",
      "source": "tokenized_tweet = all_text.apply(lambda x: x.split())\ntokenized_tweet.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "10161c3fe5bedc03a364624c91d9924c75eca5af"
      },
      "cell_type": "code",
      "source": "stemmer = PorterStemmer()\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "755d48555641b0fbe561d0e88dca86707bef06ef"
      },
      "cell_type": "code",
      "source": "for i in range(1, len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ca478a2e4c3f6a7e4761766db6f664a84f9e3587"
      },
      "cell_type": "code",
      "source": "tokenized_tweet.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "81867d9852c0ed9f5a45dec449855e55c85ad0cf"
      },
      "cell_type": "code",
      "source": "all_text = tokenized_tweet.apply(lambda x: str(x))\ntrain_text = all_text[:train.shape[0]]\ntest_text = all_text[train.shape[0]:]\nall_text.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e8da7f9f1a173ff3b786f0faafff01758282dc9"
      },
      "cell_type": "code",
      "source": "# First on real words\nword_vectorizer = TfidfVectorizer(\n    sublinear_tf=True,\n    strip_accents='unicode',\n    analyzer='word',\n    token_pattern=r'\\w{1,}',\n    stop_words='english',\n    ngram_range=(1, 2),\n    max_features=20000)\n\nword_vectorizer.fit(all_text)\ntrain_word_features = word_vectorizer.transform(train_text)\ntest_word_features = word_vectorizer.transform(test_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0f9324c52f54ad942ca39cf1183bef444cec248f"
      },
      "cell_type": "code",
      "source": "all_text = all_text.apply(lambda x: str(x))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a8186e616d201ab40ca174de44563ef886ba707c"
      },
      "cell_type": "markdown",
      "source": "## Prepare numerical features"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "50091c0a13bceda478c4ed332104554c3d7f55b0"
      },
      "cell_type": "code",
      "source": "# Scaling numerical features with MinMaxScaler though tree boosters don't need that\nnum_features = [f_ for f_ in df.columns if f_ not in [\"tweet\", \"tidy_tweet\"]]\n\nskl = MinMaxScaler()\ntrain_num_features = csr_matrix(skl.fit_transform(df.iloc[:train.shape[0], :][num_features]))\ntest_num_features = csr_matrix(skl.fit_transform(df.iloc[train.shape[0]:, :][num_features]))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae3bf15865cefb7eb49391e8780795478e6b348a"
      },
      "cell_type": "markdown",
      "source": "## Stack TF-IDF Matrixes"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1f8cf0790a336634ba29dcd15816450f85048347"
      },
      "cell_type": "code",
      "source": "csr_trn = hstack(\n            [\n                train_char_features,\n                train_word_features,\n                train_num_features\n            ]\n        ).tocsr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2afe493f98d1ede08271ba565914d75f08a5df9f"
      },
      "cell_type": "code",
      "source": "csr_sub = hstack(\n            [\n                test_char_features,\n                test_word_features,\n                test_num_features\n            ]\n        ).tocsr()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a4b35378fa82aaa3fdf95f6350e6381cfe777b1b"
      },
      "cell_type": "markdown",
      "source": "## Logistic Regression"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "61123736c0e8ac69950f5e9bf2412edbd4782692"
      },
      "cell_type": "code",
      "source": "scores = []\nfolds = KFold(n_splits=8, shuffle=True, random_state=42)\ntrain_pred = np.zeros(train.shape[0])\nprediction = np.zeros(test.shape[0])\ntrain_target = train.label",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e19269a6369d060bf2d1e47780488471be6c24b1"
      },
      "cell_type": "code",
      "source": "# Set regression parameters\nall_parameters = {\n    'C': 1,\n    'tol': 0.1,\n    'solver': 'lbfgs',\n    'fit_intercept':True,\n    'penalty': 'l2',\n    'class_weight': 'balanced',\n    'verbose': 0\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5dc0ccfa4f6794aa67b8198ad5ba10b71fb6d978"
      },
      "cell_type": "code",
      "source": "classifier = LogisticRegression(**all_parameters)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "733ec15e9e904919a391e4f6dad5193542dad4c9"
      },
      "cell_type": "code",
      "source": "trn_idx = list(enumerate(folds.split(train, train_target)))[0][1][0]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08e986554c8ba1506b24c1867fe6fdbbaedf44ed",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "train_pred = np.zeros(train.shape[0])\nprediction = np.zeros(test.shape[0])\ntrain_target = train.label\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, train_target)):\n    # Train LR\n    classifier.fit(csr_trn[trn_idx], train_target.values[trn_idx])\n    train_pred[val_idx] = classifier.predict_proba(csr_trn[val_idx])[:, 1]\n    prediction += classifier.predict_proba(csr_sub)[:,1] / folds.n_splits\n    rscore = roc_auc_score(train_target.values[val_idx], train_pred[val_idx])\n    \n    print(\"\\t Fold %d : %.6f ROC AUC\" % (n_fold + 1, rscore))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c387f942090ce4b5edc5f2298452b9b7c9d084d2"
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % roc_auc_score(train_target, train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "01eb00b7eccbb970e513b151b40de22bc8248b17"
      },
      "cell_type": "code",
      "source": "train_pred_lr = train_pred.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eefdfd30dcc21a33e063c0a8f674ed22a4746942"
      },
      "cell_type": "code",
      "source": "precision, recall, tresh = precision_recall_curve(train_target, train_pred)\nf1 = []\nfor t in range(len(tresh)):\n    f1.append(2/((1/precision[t])+(1/recall[t])))\nth = tresh[np.argmax(f1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "296053df8a236055306c386f85f3c3db399e460e"
      },
      "cell_type": "code",
      "source": "train_prediction = train_pred >= th\ntrain_prediction = train_prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be9ac777f049cbe9804ec475acce5bde001c3f9"
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % f1_score(train_target, train_prediction))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7db398280692e0ce3152fdeba7a20f08c4eb3b75"
      },
      "cell_type": "code",
      "source": "prediction_lr = prediction.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d90f9882e93db40f35d9a57742f75ad379c4917b"
      },
      "cell_type": "code",
      "source": "prediction = prediction - min(prediction)\nprediction = prediction/max(prediction)\nprediction = prediction >= th\nprediction = prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6eb0a22e4162658528a1c9cec0cd3e8364ed6d9"
      },
      "cell_type": "code",
      "source": "submission['label'] = prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa72e2aebf17e88bcea416414f4f1dfc89ba62a"
      },
      "cell_type": "code",
      "source": "submission.to_csv('lr_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83cf3ca8ea95d829e0856f892f3e1cc7fd19275d"
      },
      "cell_type": "markdown",
      "source": "## LightGBM"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f1de2bd10225372db82a1b0cbe73e2c9f1091503"
      },
      "cell_type": "code",
      "source": "folds = KFold(n_splits=8, shuffle=True, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e19269a6369d060bf2d1e47780488471be6c24b1"
      },
      "cell_type": "code",
      "source": "# Set LGBM parameters\nparams = {\n    \"objective\": \"binary\",\n    'metric': {'auc'},\n    \"boosting_type\": \"gbdt\",\n    \"verbosity\": -1,\n    \"num_threads\": 4,\n    \"bagging_fraction\": 0.8,\n    \"feature_fraction\": 0.8,\n    \"learning_rate\": 0.05,\n    \"num_leaves\": 64,\n    \"verbose\": -1,\n    \"min_split_gain\": .1,\n    \"reg_alpha\": .1,\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baa5598c4e543ef311c8bdb6c40222728cb7c3b2"
      },
      "cell_type": "code",
      "source": "lgb_round_dict = defaultdict(int)\ntrn_lgbset = lgb.Dataset(csr_trn, free_raw_data=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08e986554c8ba1506b24c1867fe6fdbbaedf44ed",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "train_pred = np.zeros(train.shape[0])\nprediction = np.zeros(test.shape[0])\ntrn_lgbset.set_label(train_target.values)\nlgb_rounds = 5000\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, train_target)):\n    watchlist = [\n    trn_lgbset.subset(trn_idx),\n    trn_lgbset.subset(val_idx)\n    ]\n    # Train lgb l1\n    model = lgb.train(\n        params=params,\n        train_set=watchlist[0],\n        num_boost_round=lgb_rounds,\n        valid_sets=watchlist,\n        early_stopping_rounds=50,\n        verbose_eval=0\n    )\n    train_pred[val_idx] = model.predict(trn_lgbset.data[val_idx], num_iteration=model.best_iteration)\n    prediction += model.predict(csr_sub, \n                                    num_iteration = model.best_iteration) / folds.n_splits\n    rscore = roc_auc_score(train_target.values[val_idx], train_pred[val_idx])\n    \n    print(\"\\t Fold %d : %.6f ROC AUC in %3d rounds\" % (n_fold + 1, rscore, model.best_iteration))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6480b35dd62890662c1d6117d5aad6ff5e2a6ae8"
      },
      "cell_type": "code",
      "source": "train_pred_lgb = train_pred.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c387f942090ce4b5edc5f2298452b9b7c9d084d2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % roc_auc_score(train_target, train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eefdfd30dcc21a33e063c0a8f674ed22a4746942"
      },
      "cell_type": "code",
      "source": "precision, recall, tresh = precision_recall_curve(train_target, train_pred)\nf1 = []\nfor t in range(len(tresh)):\n    f1.append(2/((1/precision[t])+(1/recall[t])))\nth = tresh[np.argmax(f1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "296053df8a236055306c386f85f3c3db399e460e"
      },
      "cell_type": "code",
      "source": "train_prediction = train_pred >= th\ntrain_prediction = train_prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be9ac777f049cbe9804ec475acce5bde001c3f9"
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % f1_score(train_target, train_prediction))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14d98a5cac7a4edfea933ac4f5024567ddeb172f"
      },
      "cell_type": "code",
      "source": "prediction_lgb = prediction.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d90f9882e93db40f35d9a57742f75ad379c4917b"
      },
      "cell_type": "code",
      "source": "prediction = prediction - min(prediction)\nprediction = prediction/max(prediction)\nprediction = prediction >= th\nprediction = prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6eb0a22e4162658528a1c9cec0cd3e8364ed6d9"
      },
      "cell_type": "code",
      "source": "submission['label'] = prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa72e2aebf17e88bcea416414f4f1dfc89ba62a"
      },
      "cell_type": "code",
      "source": "submission.to_csv('lgbm_stem_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "83cf3ca8ea95d829e0856f892f3e1cc7fd19275d"
      },
      "cell_type": "markdown",
      "source": "## XGBoost"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "975c87494aa129108de25822ebd44e9784d85d3d"
      },
      "cell_type": "code",
      "source": "folds = KFold(n_splits=8, shuffle=True, random_state=42)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e19269a6369d060bf2d1e47780488471be6c24b1"
      },
      "cell_type": "code",
      "source": "# Set XGBoost parameters\nparam = {}\nparam['objective'] = 'binary:logistic'\nparam['eta'] = 0.1\nparam['max_depth'] = 6\nparam['silent'] = 1\nparam['eval_metric'] = 'auc'\nparam['min_child_weight'] = 1\nparam['subsample'] = 0.7\nparam['colsample_bytree'] = 0.7\nparam['seed'] = 23\n\nplst = list(param.items())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "baa5598c4e543ef311c8bdb6c40222728cb7c3b2"
      },
      "cell_type": "code",
      "source": "xgtrain = xgb.DMatrix(csr_trn, label=train.label.values)\nxgtest = xgb.DMatrix(csr_sub)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "08e986554c8ba1506b24c1867fe6fdbbaedf44ed",
        "scrolled": false
      },
      "cell_type": "code",
      "source": "train_pred = np.zeros(train.shape[0])\nprediction = np.zeros(test.shape[0])\nnum_rounds = 500\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(train, train_target)):\n    watchlist = [ (xgtrain.slice(trn_idx),'train'), (xgtrain.slice(val_idx), 'test') ]\n    # Train xgb\n    model = xgb.train(\n        params = plst,\n        dtrain = xgtrain.slice(trn_idx),\n        num_boost_round = 500,\n        evals = watchlist,\n        early_stopping_rounds=20,\n        verbose_eval=50\n    )\n    \n    train_pred[val_idx] = model.predict(xgtrain.slice(val_idx), ntree_limit = model.best_ntree_limit)\n    prediction += model.predict(xgtest, ntree_limit = model.best_ntree_limit) / folds.n_splits\n    rscore = roc_auc_score(train_target.values[val_idx], train_pred[val_idx])\n    \n    print(\"\\t Fold %d : %.6f ROC AUC in %3d rounds\" % (n_fold + 1, rscore, model.best_iteration))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6480b35dd62890662c1d6117d5aad6ff5e2a6ae8"
      },
      "cell_type": "code",
      "source": "train_pred_xgb = train_pred.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c387f942090ce4b5edc5f2298452b9b7c9d084d2",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % roc_auc_score(train_target, train_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eefdfd30dcc21a33e063c0a8f674ed22a4746942"
      },
      "cell_type": "code",
      "source": "precision, recall, tresh = precision_recall_curve(train_target, train_pred)\nf1 = []\nfor t in range(len(tresh)):\n    f1.append(2/((1/precision[t])+(1/recall[t])))\nth = tresh[np.argmax(f1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "296053df8a236055306c386f85f3c3db399e460e"
      },
      "cell_type": "code",
      "source": "train_prediction = train_pred >= th\ntrain_prediction = train_prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be9ac777f049cbe9804ec475acce5bde001c3f9"
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % f1_score(train_target, train_prediction))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "14d98a5cac7a4edfea933ac4f5024567ddeb172f"
      },
      "cell_type": "code",
      "source": "prediction_xgb = prediction.copy()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d90f9882e93db40f35d9a57742f75ad379c4917b"
      },
      "cell_type": "code",
      "source": "prediction = prediction - min(prediction)\nprediction = prediction/max(prediction)\nprediction = prediction >= th\nprediction = prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c6eb0a22e4162658528a1c9cec0cd3e8364ed6d9"
      },
      "cell_type": "code",
      "source": "submission['label'] = prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1fa72e2aebf17e88bcea416414f4f1dfc89ba62a"
      },
      "cell_type": "code",
      "source": "submission.to_csv('xgb_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "16d356ef3a4a0d7be9659dd6d4745a38e22c56b2"
      },
      "cell_type": "markdown",
      "source": "## Cray Ensemble"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8d13c248ec97840c421cc8c0b5e944089b6e28d"
      },
      "cell_type": "code",
      "source": "train_pred = np.power((train_pred_lr * train_pred_xgb * train_pred_lgb), 1/3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "eefdfd30dcc21a33e063c0a8f674ed22a4746942"
      },
      "cell_type": "code",
      "source": "precision, recall, tresh = precision_recall_curve(train_target, train_pred)\nf1 = []\nfor t in range(len(tresh)):\n    f1.append(2/((1/precision[t])+(1/recall[t])))\nth = tresh[np.argmax(f1)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "296053df8a236055306c386f85f3c3db399e460e"
      },
      "cell_type": "code",
      "source": "train_prediction = train_pred >= th\ntrain_prediction = train_prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1be9ac777f049cbe9804ec475acce5bde001c3f9"
      },
      "cell_type": "code",
      "source": "print(\"full score : %.6f\" % f1_score(train_target, train_prediction))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3eef0b4898835c2e082c70fc06d9aa2cb403f02"
      },
      "cell_type": "code",
      "source": "prediction = np.power((prediction_lgb * prediction_xgb * prediction_lr), 1/3)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "474e446cabce1df3eb8c8634ee543d014feffd8f"
      },
      "cell_type": "code",
      "source": "prediction = prediction - min(prediction)\nprediction = prediction/max(prediction)\nprediction = prediction >= th\nprediction = prediction.astype(int)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "16576c2ee010bfc1bd1cc0ff71f4604a78d78b55"
      },
      "cell_type": "code",
      "source": "submission['label'] = prediction",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c160e6ef8c3d4aaac3479aec24b884f6f79252f6"
      },
      "cell_type": "code",
      "source": "submission.to_csv('ensemble_submission.csv', index=False)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}